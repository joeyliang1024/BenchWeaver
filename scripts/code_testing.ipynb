{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MCQA Criteria Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '''你是一個評估模型，將被給予一個選擇題問題、一個 LLM 回應，以及一個選項 {option}。\n",
    "你的任務不是回答問題，而是判斷 LLM 回應中是否 明確選擇 {option} 作為答案之一。\n",
    "\n",
    "問題: {question}\n",
    "\n",
    "LLM 回應: {llm_response}\n",
    "\n",
    "判斷標準：\n",
    "\n",
    "如果 {option} 在 LLM 回應中被清楚且直接表達為選擇的答案，或 LLM 回應僅含選項(A, B, C, D等)或本身 ({option})，則請回答 'True'。\n",
    "\n",
    "如果 {option} 未被選擇，或 LLM 回應未表達出明確的答案，請回答 'False'。\n",
    "\n",
    "如果 LLM 回應為空，或其內容無法確定 {option} 是否為選擇的答案，請回答 'Unknown'。\n",
    "'''\n",
    "text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '''你是一个评估模型，将被给予一个选择题问题、一个 LLM 回应，以及一个选项 {option}。  \n",
    "你的任务不是回答问题，而是判断 LLM 回应中是否明确选择 {option} 作为答案之一。  \n",
    "\n",
    "问题: {question}  \n",
    "\n",
    "LLM 回应: {llm_response}  \n",
    "\n",
    "判断标准：  \n",
    "\n",
    "如果 {option} 在 LLM 回应中被清楚且直接表达为选择的答案，或 LLM 回应仅包含选项（A、B、C、D 等）或本身（{option}），则请回答 'True'。  \n",
    "\n",
    "如果 {option} 未被选择，或 LLM 回应未表达出明确的答案，请回答 'False'。  \n",
    "\n",
    "如果 LLM 回应为空，或其内容无法确定 {option} 是否为选择的答案，请回答 'Unknown'。  \n",
    "\n",
    "'''\n",
    "text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '''당신은 평가 모델이며, 하나의 객관식 질문, LLM 응답, 그리고 하나의 선택지 {option}을 받게 됩니다.  \n",
    "당신의 임무는 질문에 답하는 것이 아니라, LLM 응답에서 {option}이 명확하게 선택된 답변 중 하나인지 판단하는 것입니다.  \n",
    "\n",
    "질문: {question}  \n",
    "\n",
    "LLM 응답: {llm_response}  \n",
    "\n",
    "판단 기준:  \n",
    "\n",
    "{option}이 LLM 응답에서 명확하고 직접적으로 선택된 답변으로 표현되었거나, LLM 응답이 선택지(A, B, C, D 등) 또는 {option}만 포함하는 경우 'True'를 답하십시오.  \n",
    "\n",
    "{option}이 선택되지 않았거나, LLM 응답이 명확한 답을 표현하지 않았다면 'False'를 답하십시오.  \n",
    "\n",
    "LLM 응답이 비어 있거나, {option}이 선택된 답변인지 판단할 수 없다면 'Unknown'을 답하십시오.  \n",
    "\n",
    "'''\n",
    "text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '''You are an evaluation model and will be given a multiple-choice question, an LLM response, and an option {option}.  \n",
    "Your task is not to answer the question but to determine whether the LLM response explicitly selects {option} as one of the answers.  \n",
    "\n",
    "Question: {question}  \n",
    "\n",
    "LLM Response: {llm_response}  \n",
    "\n",
    "Evaluation criteria:  \n",
    "\n",
    "If {option} is clearly and directly expressed as a selected answer in the LLM response, or if the LLM response contains only an option (A, B, C, D, etc.) or {option} itself, respond with 'True'.  \n",
    "\n",
    "If {option} is not selected, or the LLM response does not clearly express an answer, respond with 'False'.  \n",
    "\n",
    "If the LLM response is empty or it is unclear whether {option} is a selected answer, respond with 'Unknown'.  \n",
    "\n",
    "'''\n",
    "text.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Translation Criteria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import AzureOpenAI\n",
    "from BenchWeaver.extras.load_env import load_env_variables\n",
    "\n",
    "load_env_variables()\n",
    "client = AzureOpenAI(\n",
    "    azure_endpoint=os.getenv(\"AZURE_ENDPOINT_URL\"),\n",
    "    api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),\n",
    "    api_version=os.getenv(\"AZURE_API_VERSION\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criteria_prompt = '''\n",
    "你是一位專業的翻譯評估員。請根據提供的「原文文本」、「翻譯文本」、「風格範例」及「評估標準」，對翻譯品質進行評估，評分範圍為 1（最差）至 10（最佳），並以 JSON 格式輸出結果。\n",
    "\n",
    "評估標準：\n",
    "1. 資訊保留度：\n",
    "   - 評估翻譯文本是否完整保留了原文的資訊內容，包括關鍵細節、邏輯關係與語義準確性。\n",
    "   - 允許因風格匹配的需求進行詞句調整，但不可影響核心資訊的傳遞。\n",
    "   - 例如，若原文提及具體數據、時間、因果關係或條件，翻譯文本應忠實呈現，而非省略或改動這些重要內容。\n",
    "   - 若翻譯文本有刪減、曲解或誤譯，則應降低分數。\n",
    "\n",
    "2. 風格匹配度：\n",
    "   - 若風格範例為空，則請直接給予 1 分。\n",
    "   - 評估翻譯文本是否符合給定的「風格範例」，包括語氣、句式、措辭選擇、正式度等。\n",
    "   - 例如，若風格範例是學術論文，則翻譯文本應使用正式、嚴謹的語言，避免口語化表達；若風格範例是兒童讀物，則應使用簡單易懂、富有親和力的詞彙。\n",
    "   - 風格匹配度高的翻譯應該讀起來與範例文本的風格一致，而不只是逐字翻譯。\n",
    "\n",
    "3. 專有名詞準確度：\n",
    "   - 專有名詞包括人名、地名、機構名稱、術語、技術詞彙等，應與上下文一致，並符合標準翻譯慣例。\n",
    "   - 例如，「United Nations」應翻譯為「聯合國」，而非「統一國家」；「Neural Network」應譯為「神經網絡」，而非「神經連接」。\n",
    "   - 若專有名詞有公認的譯法，則應使用標準譯法，若無固定譯法，則應確保譯法在全文內保持一致。\n",
    "\n",
    "4. 翻譯品質：\n",
    "   - 綜合評估翻譯文本的整體品質，包括語法、流暢度與可讀性。\n",
    "   - 翻譯應避免生硬直譯或機翻痕跡，確保句子通順自然、符合目標語言的語法規範。\n",
    "   - 例如，若翻譯文本讀起來拗口或不符合語法，應降低分數；若譯文自然流暢，則應提高分數。\n",
    "\n",
    "---\n",
    "「原文文本」：\n",
    "{source_text}\n",
    "\n",
    "「翻譯文本」：\n",
    "{target_text}\n",
    "\n",
    "「風格範例」：\n",
    "{style_example}\n",
    "---\n",
    "\n",
    "請以以下 JSON 格式輸出評估結果，確保 `分數` 為 1-10 之間的整數，`原因` 為簡要但具體的說明：\n",
    "{\n",
    "    \"資訊保留度\": {\n",
    "        \"分數\": <1-10 的分數>,\n",
    "        \"原因\": \"<簡要說明此評分的理由>\"\n",
    "    },\n",
    "    \"風格匹配度\": {\n",
    "        \"分數\": <1-10 的分數>,\n",
    "        \"原因\": \"<簡要說明此評分的理由>\"\n",
    "    },\n",
    "    \"專有名詞準確度\": {\n",
    "        \"分數\": <1-10 的分數>,\n",
    "        \"原因\": \"<簡要說明此評分的理由>\"\n",
    "    },\n",
    "    \"翻譯品質\": {\n",
    "        \"分數\": <1-10 的分數>,\n",
    "        \"原因\": \"<簡要說明此評分的理由>\"\n",
    "    }\n",
    "}\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translation = \"以下是關於會計的選擇題（及答案）。\\n\\n如果在期末將商品存貨金額錯誤地記錄為560,000元而實際為650,000元，對當期的銷售成本和當期淨利的影響正確的是？（假設存貨資產評價採用實地存貨調查法。）\\nA. （銷售成本）過高，（當期淨利）過低\\nB. （銷售成本）過高，（當期淨利）過高\\nC. （銷售成本）過低，（當期淨利）過低\\nD. （銷售成本）過低，（當期淨利）過高\\n正確答案：\"\n",
    "original_text = \"다음은 accounting에 대한 객관식 질문(및 정답)입니다.\\n\\n전기 말에 상품재고액 \\\\560,000을 \\\\650,000으로 잘못 계상한 경우, 당기의 매출원가와 당기순이익에 미치는 영향으로 옳은 것은? (단, 재고자산 평가는 실지재고조사법을 적용 한다.)\\nA. (매출원가) 과대, (당기순이익) 과소\\nB. (매출원가) 과대, (당기순이익) 과대\\nC. (매출원가) 과소, (당기순이익) 과소\\nD. (매출원가) 과소, (당기순이익) 과대\\n정답:\"\n",
    "style_example = \"\"\"\n",
    "\"\"\"\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\", \n",
    "        \"content\": \"you are a helpful assistant.\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\", \n",
    "        \"content\": criteria_prompt.replace(\"{source_text}\", original_text).replace(\"{target_text}\", translation).replace(\"{style_example}\", style_example).strip()\n",
    "    }\n",
    "    ]\n",
    "print(messages[-1]['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import BadRequestError\n",
    "\n",
    "try:\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=messages,\n",
    "    )\n",
    "    try: \n",
    "        import ast\n",
    "        import json\n",
    "        import re\n",
    "        resp_dict = ast.literal_eval(re.sub(r'\\\\|\\n', '', response.choices[0].message.content))\n",
    "        print(json.dumps(resp_dict, ensure_ascii=False, indent=2))\n",
    "    except Exception as e:\n",
    "        print(\"Error parsing response:\", e)\n",
    "        print(\"Raw response:\", response.choices[0].message.content)\n",
    "except BadRequestError as e:\n",
    "    error_dict = e.response.content.decode()\n",
    "    import ast\n",
    "    import json\n",
    "    resp_dict = ast.literal_eval(error_dict)\n",
    "    response = ast.literal_eval(error_dict)['error']['message']\n",
    "    print(json.dumps(resp_dict, indent=2))\n",
    "    print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "John went to the store.\n",
      "Mary likes ice cream.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "text = \"\"\"For example:\n",
    "John went to the store.\n",
    "Mary likes ice cream.\n",
    "\n",
    "Source sentence: This is a test.\"\"\"\n",
    "\n",
    "pattern = r\"(?:For example:|Examples:|Few-shot Examples:)\\s*(.*?)\\s*(?:Source sentence:|Proper Noun Examples:)\"\n",
    "match = re.search(pattern, text, re.DOTALL)\n",
    "\n",
    "if match:\n",
    "    extracted_text = match.group(1).strip()\n",
    "    print(extracted_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Refine translation Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# load the JSON data    \n",
    "with open('/work/u5110390/BenchWeaver/prompt/translation_prompt.json', 'r') as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export to json\n",
    "\n",
    "with open('/work/u5110390/BenchWeaver/prompt/translation_prompt.json', 'w') as f:\n",
    "    json.dump(data, f, indent=2, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Re-scoreing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import os\n",
    "from typing import Any, Dict, List\n",
    "import numpy as np\n",
    "from BenchWeaver.eval.benchmarks.configs import BENCHMARK_CONFIG\n",
    "from tqdm import tqdm\n",
    "\n",
    "def parse_bool_score(text: str) -> str:\n",
    "    '''\n",
    "    Normally for MCQA checking. The answer is either true, false or unknown.\n",
    "    '''\n",
    "    text = text.lower()\n",
    "    match = re.search(r'\\b(true|false|unknown)\\b', text)\n",
    "    return match.group(0) if match else \"\"\n",
    "        \n",
    "def compute_score(benchmark_name: str, \n",
    "                  checked_answers: Dict[str, List[Any]], \n",
    "                  check_results: Dict[str, List[Any]], \n",
    "                  mapping_dict: Dict[str, dict]\n",
    "    ) -> Dict[str, float]:\n",
    "    category_corrects = {score: {\"corrects\": 0, \"true_mask_count\": 0} for score in BENCHMARK_CONFIG[benchmark_name]['display_scores']}\n",
    "\n",
    "    for subject in tqdm(mapping_dict.keys(), desc=\"Compute subjects\"):\n",
    "        category_name = mapping_dict[subject][\"category\"]\n",
    "        \n",
    "        # Ground truth and predictions\n",
    "        answers = np.array(checked_answers[subject])\n",
    "        predictions = np.array([parse_bool_score(ans) for ans in check_results[subject]])\n",
    "\n",
    "        # Mask for when the answer is 'true'\n",
    "        true_mask = answers == 'true'\n",
    "\n",
    "        # Compare predictions and answers, only where answer is 'true'\n",
    "        corrects = (predictions == 'true') & true_mask  # correct when answer is 'true' and prediction is 'true'\n",
    "\n",
    "        # Append results to category\n",
    "        category_corrects[category_name][\"corrects\"] += corrects.sum()\n",
    "        category_corrects[category_name][\"true_mask_count\"] += true_mask.sum()\n",
    "        category_corrects[\"Average\"]['corrects'] += corrects.sum()\n",
    "        category_corrects[\"Average\"]['true_mask_count'] += true_mask.sum()\n",
    "\n",
    "    # Compute accuracy per category: correct_true / total_true\n",
    "    results = {}\n",
    "    for category_name, record_dict in category_corrects.items():\n",
    "        acc = round(100 * (record_dict['corrects'] / record_dict['true_mask_count']), 4)\n",
    "        results[category_name] = acc\n",
    "\n",
    "    return results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compute subjects: 100%|██████████| 67/67 [00:00<00:00, 1541.42it/s]\n"
     ]
    }
   ],
   "source": [
    "benchmark_name = \"cmmlu\"\n",
    "folder = f\"/work/u5110390/BenchWeaver/score/trans_template_exp/{benchmark_name}/origin_lang\"\n",
    "mapping_path = os.path.join(f\"/work/u5110390/BenchWeaver/evaluation_data/{benchmark_name}/mapping.json\")\n",
    "\n",
    "# load the JSON data    \n",
    "with open(mapping_path, 'r') as f:\n",
    "    mapping = json.load(f)\n",
    "    \n",
    "with open(os.path.join(folder, \"checked_answers.json\"), 'r') as f:\n",
    "    answer_data = json.load(f)\n",
    "    \n",
    "with open(os.path.join(folder, \"check_results.json\"), 'r') as f:\n",
    "    checked_data = json.load(f)\n",
    "    # retreive the answer\n",
    "    for subj, check_list in checked_data.items():\n",
    "        bool_list = []\n",
    "        for check in check_list:\n",
    "            bool_list.append(parse_bool_score(check))\n",
    "        checked_data[subj] = bool_list\n",
    "\n",
    "score_dict = compute_score(benchmark_name, answer_data, checked_data, mapping)\n",
    "\n",
    "with open(os.path.join(folder, \"score.json\"), 'w') as f:\n",
    "    json.dump(score_dict, f, indent=4, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compute subjects: 100%|██████████| 66/66 [00:00<00:00, 899.69it/s]\n"
     ]
    }
   ],
   "source": [
    "benchmark_name = \"tmmluplus\"\n",
    "folder = f\"/work/u5110390/BenchWeaver/score/trans_template_exp/{benchmark_name}/origin_lang\"\n",
    "mapping_path = os.path.join(f\"/work/u5110390/BenchWeaver/evaluation_data/{benchmark_name}/mapping.json\")\n",
    "\n",
    "# load the JSON data    \n",
    "with open(mapping_path, 'r') as f:\n",
    "    mapping = json.load(f)\n",
    "    \n",
    "with open(os.path.join(folder, \"checked_answers.json\"), 'r') as f:\n",
    "    answer_data = json.load(f)\n",
    "    \n",
    "with open(os.path.join(folder, \"check_results.json\"), 'r') as f:\n",
    "    checked_data = json.load(f)\n",
    "    # retreive the answer\n",
    "    for subj, check_list in checked_data.items():\n",
    "        bool_list = []\n",
    "        for check in check_list:\n",
    "            bool_list.append(parse_bool_score(check))\n",
    "        checked_data[subj] = bool_list\n",
    "\n",
    "score_dict = compute_score(benchmark_name, answer_data, checked_data, mapping)\n",
    "\n",
    "with open(os.path.join(folder, \"score.json\"), 'w') as f:\n",
    "    json.dump(score_dict, f, indent=4, ensure_ascii=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BenchWeaver",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
