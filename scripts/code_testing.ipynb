{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MCQA Criteria Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '''你是一個評估模型，將被給予一個選擇題問題、一個 LLM 回應，以及一個選項 {option}。\n",
    "你的任務不是回答問題，而是判斷 LLM 回應中是否 明確選擇 {option} 作為答案之一。\n",
    "\n",
    "問題: {question}\n",
    "\n",
    "LLM 回應: {llm_response}\n",
    "\n",
    "判斷標準：\n",
    "\n",
    "如果 {option} 在 LLM 回應中被清楚且直接表達為選擇的答案，或 LLM 回應僅含選項(A, B, C, D等)或本身 ({option})，則請回答 'True'。\n",
    "\n",
    "如果 {option} 未被選擇，或 LLM 回應未表達出明確的答案，請回答 'False'。\n",
    "\n",
    "如果 LLM 回應為空，或其內容無法確定 {option} 是否為選擇的答案，請回答 'Unknown'。\n",
    "'''\n",
    "text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '''你是一个评估模型，将被给予一个选择题问题、一个 LLM 回应，以及一个选项 {option}。  \n",
    "你的任务不是回答问题，而是判断 LLM 回应中是否明确选择 {option} 作为答案之一。  \n",
    "\n",
    "问题: {question}  \n",
    "\n",
    "LLM 回应: {llm_response}  \n",
    "\n",
    "判断标准：  \n",
    "\n",
    "如果 {option} 在 LLM 回应中被清楚且直接表达为选择的答案，或 LLM 回应仅包含选项（A、B、C、D 等）或本身（{option}），则请回答 'True'。  \n",
    "\n",
    "如果 {option} 未被选择，或 LLM 回应未表达出明确的答案，请回答 'False'。  \n",
    "\n",
    "如果 LLM 回应为空，或其内容无法确定 {option} 是否为选择的答案，请回答 'Unknown'。  \n",
    "\n",
    "'''\n",
    "text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '''당신은 평가 모델이며, 하나의 객관식 질문, LLM 응답, 그리고 하나의 선택지 {option}을 받게 됩니다.  \n",
    "당신의 임무는 질문에 답하는 것이 아니라, LLM 응답에서 {option}이 명확하게 선택된 답변 중 하나인지 판단하는 것입니다.  \n",
    "\n",
    "질문: {question}  \n",
    "\n",
    "LLM 응답: {llm_response}  \n",
    "\n",
    "판단 기준:  \n",
    "\n",
    "{option}이 LLM 응답에서 명확하고 직접적으로 선택된 답변으로 표현되었거나, LLM 응답이 선택지(A, B, C, D 등) 또는 {option}만 포함하는 경우 'True'를 답하십시오.  \n",
    "\n",
    "{option}이 선택되지 않았거나, LLM 응답이 명확한 답을 표현하지 않았다면 'False'를 답하십시오.  \n",
    "\n",
    "LLM 응답이 비어 있거나, {option}이 선택된 답변인지 판단할 수 없다면 'Unknown'을 답하십시오.  \n",
    "\n",
    "'''\n",
    "text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '''You are an evaluation model and will be given a multiple-choice question, an LLM response, and an option {option}.  \n",
    "Your task is not to answer the question but to determine whether the LLM response explicitly selects {option} as one of the answers.  \n",
    "\n",
    "Question: {question}  \n",
    "\n",
    "LLM Response: {llm_response}  \n",
    "\n",
    "Evaluation criteria:  \n",
    "\n",
    "If {option} is clearly and directly expressed as a selected answer in the LLM response, or if the LLM response contains only an option (A, B, C, D, etc.) or {option} itself, respond with 'True'.  \n",
    "\n",
    "If {option} is not selected, or the LLM response does not clearly express an answer, respond with 'False'.  \n",
    "\n",
    "If the LLM response is empty or it is unclear whether {option} is a selected answer, respond with 'Unknown'.  \n",
    "\n",
    "'''\n",
    "text.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Translation Criteria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import AzureOpenAI\n",
    "from BenchWeaver.extras.load_env import load_env_variables\n",
    "\n",
    "load_env_variables()\n",
    "client = AzureOpenAI(\n",
    "    azure_endpoint=os.getenv(\"AZURE_ENDPOINT_URL\"),\n",
    "    api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),\n",
    "    api_version=os.getenv(\"AZURE_API_VERSION\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criteria_prompt = '''\n",
    "你是一位專業的翻譯評估員。請根據提供的「原文文本」、「翻譯文本」、「風格範例」及「評估標準」，對翻譯品質進行評估，評分範圍為 1（最差）至 10（最佳），並以 JSON 格式輸出結果。\n",
    "\n",
    "評估標準：\n",
    "1. 資訊保留度：\n",
    "   - 評估翻譯文本是否完整保留了原文的資訊內容，包括關鍵細節、邏輯關係與語義準確性。\n",
    "   - 允許因風格匹配的需求進行詞句調整，但不可影響核心資訊的傳遞。\n",
    "   - 例如，若原文提及具體數據、時間、因果關係或條件，翻譯文本應忠實呈現，而非省略或改動這些重要內容。\n",
    "   - 若翻譯文本有刪減、曲解或誤譯，則應降低分數。\n",
    "\n",
    "2. 風格匹配度：\n",
    "   - 若風格範例為空，則請直接給予 1 分。\n",
    "   - 評估翻譯文本是否符合給定的「風格範例」，包括語氣、句式、措辭選擇、正式度等。\n",
    "   - 例如，若風格範例是學術論文，則翻譯文本應使用正式、嚴謹的語言，避免口語化表達；若風格範例是兒童讀物，則應使用簡單易懂、富有親和力的詞彙。\n",
    "   - 風格匹配度高的翻譯應該讀起來與範例文本的風格一致，而不只是逐字翻譯。\n",
    "\n",
    "3. 專有名詞準確度：\n",
    "   - 專有名詞包括人名、地名、機構名稱、術語、技術詞彙等，應與上下文一致，並符合標準翻譯慣例。\n",
    "   - 例如，「United Nations」應翻譯為「聯合國」，而非「統一國家」；「Neural Network」應譯為「神經網絡」，而非「神經連接」。\n",
    "   - 若專有名詞有公認的譯法，則應使用標準譯法，若無固定譯法，則應確保譯法在全文內保持一致。\n",
    "\n",
    "4. 翻譯品質：\n",
    "   - 綜合評估翻譯文本的整體品質，包括語法、流暢度與可讀性。\n",
    "   - 翻譯應避免生硬直譯或機翻痕跡，確保句子通順自然、符合目標語言的語法規範。\n",
    "   - 例如，若翻譯文本讀起來拗口或不符合語法，應降低分數；若譯文自然流暢，則應提高分數。\n",
    "\n",
    "---\n",
    "「原文文本」：\n",
    "{source_text}\n",
    "\n",
    "「翻譯文本」：\n",
    "{target_text}\n",
    "\n",
    "「風格範例」：\n",
    "{style_example}\n",
    "---\n",
    "\n",
    "請以以下 JSON 格式輸出評估結果，確保 `分數` 為 1-10 之間的整數，`原因` 為簡要但具體的說明：\n",
    "{\n",
    "    \"資訊保留度\": {\n",
    "        \"分數\": <1-10 的分數>,\n",
    "        \"原因\": \"<簡要說明此評分的理由>\"\n",
    "    },\n",
    "    \"風格匹配度\": {\n",
    "        \"分數\": <1-10 的分數>,\n",
    "        \"原因\": \"<簡要說明此評分的理由>\"\n",
    "    },\n",
    "    \"專有名詞準確度\": {\n",
    "        \"分數\": <1-10 的分數>,\n",
    "        \"原因\": \"<簡要說明此評分的理由>\"\n",
    "    },\n",
    "    \"翻譯品質\": {\n",
    "        \"分數\": <1-10 的分數>,\n",
    "        \"原因\": \"<簡要說明此評分的理由>\"\n",
    "    }\n",
    "}\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translation = \"以下是關於會計的選擇題（及答案）。\\n\\n如果在期末將商品存貨金額錯誤地記錄為560,000元而實際為650,000元，對當期的銷售成本和當期淨利的影響正確的是？（假設存貨資產評價採用實地存貨調查法。）\\nA. （銷售成本）過高，（當期淨利）過低\\nB. （銷售成本）過高，（當期淨利）過高\\nC. （銷售成本）過低，（當期淨利）過低\\nD. （銷售成本）過低，（當期淨利）過高\\n正確答案：\"\n",
    "original_text = \"다음은 accounting에 대한 객관식 질문(및 정답)입니다.\\n\\n전기 말에 상품재고액 \\\\560,000을 \\\\650,000으로 잘못 계상한 경우, 당기의 매출원가와 당기순이익에 미치는 영향으로 옳은 것은? (단, 재고자산 평가는 실지재고조사법을 적용 한다.)\\nA. (매출원가) 과대, (당기순이익) 과소\\nB. (매출원가) 과대, (당기순이익) 과대\\nC. (매출원가) 과소, (당기순이익) 과소\\nD. (매출원가) 과소, (당기순이익) 과대\\n정답:\"\n",
    "style_example = \"\"\"\n",
    "\"\"\"\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\", \n",
    "        \"content\": \"you are a helpful assistant.\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\", \n",
    "        \"content\": criteria_prompt.replace(\"{source_text}\", original_text).replace(\"{target_text}\", translation).replace(\"{style_example}\", style_example).strip()\n",
    "    }\n",
    "    ]\n",
    "print(messages[-1]['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import BadRequestError\n",
    "\n",
    "try:\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=messages,\n",
    "    )\n",
    "    try: \n",
    "        import ast\n",
    "        import json\n",
    "        import re\n",
    "        resp_dict = ast.literal_eval(re.sub(r'\\\\|\\n', '', response.choices[0].message.content))\n",
    "        print(json.dumps(resp_dict, ensure_ascii=False, indent=2))\n",
    "    except Exception as e:\n",
    "        print(\"Error parsing response:\", e)\n",
    "        print(\"Raw response:\", response.choices[0].message.content)\n",
    "except BadRequestError as e:\n",
    "    error_dict = e.response.content.decode()\n",
    "    import ast\n",
    "    import json\n",
    "    resp_dict = ast.literal_eval(error_dict)\n",
    "    response = ast.literal_eval(error_dict)['error']['message']\n",
    "    print(json.dumps(resp_dict, indent=2))\n",
    "    print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "John went to the store.\n",
      "Mary likes ice cream.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "text = \"\"\"For example:\n",
    "John went to the store.\n",
    "Mary likes ice cream.\n",
    "\n",
    "Source sentence: This is a test.\"\"\"\n",
    "\n",
    "pattern = r\"(?:For example:|Examples:|Few-shot Examples:)\\s*(.*?)\\s*(?:Source sentence:|Proper Noun Examples:)\"\n",
    "match = re.search(pattern, text, re.DOTALL)\n",
    "\n",
    "if match:\n",
    "    extracted_text = match.group(1).strip()\n",
    "    print(extracted_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Refine translation Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# load the JSON data    \n",
    "with open('/work/u5110390/BenchWeaver/prompt/translation_prompt.json', 'r') as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export to json\n",
    "\n",
    "with open('/work/u5110390/BenchWeaver/prompt/translation_prompt.json', 'w') as f:\n",
    "    json.dump(data, f, indent=2, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Re-scoreing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-05 13:22:22.119593: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-04-05 13:22:22.119640: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-04-05 13:22:22.121176: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-04-05 13:22:22.129561: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-04-05 13:22:23.132196: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "import os\n",
    "from typing import Any, Dict, List\n",
    "import numpy as np\n",
    "from BenchWeaver.eval.benchmarks.configs import BENCHMARK_CONFIG\n",
    "from tqdm import tqdm\n",
    "\n",
    "def parse_bool_score(text: str) -> str:\n",
    "    '''\n",
    "    Normally for MCQA checking. The answer is either true, false or unknown.\n",
    "    '''\n",
    "    text = text.lower()\n",
    "    match = re.search(r'\\b(true|false|unknown)\\b', text)\n",
    "    return match.group(0) if match else \"\"\n",
    "        \n",
    "def compute_score(benchmark_name: str, \n",
    "                  checked_answers: Dict[str, List[Any]], \n",
    "                  check_results: Dict[str, List[Any]], \n",
    "                  mapping_dict: Dict[str, dict]\n",
    "    ) -> Dict[str, float]:\n",
    "    category_corrects = {score: {\"corrects\": 0, \"true_mask_count\": 0} for score in BENCHMARK_CONFIG[benchmark_name]['display_scores']}\n",
    "\n",
    "    for subject in tqdm(mapping_dict.keys(), desc=\"Compute subjects\"):\n",
    "        category_name = mapping_dict[subject][\"category\"]\n",
    "        \n",
    "        # Ground truth and predictions\n",
    "        answers = np.array(checked_answers[subject])\n",
    "        predictions = np.array([parse_bool_score(ans) for ans in check_results[subject]])\n",
    "\n",
    "        # Mask for when the answer is 'true'\n",
    "        true_mask = answers == 'true'\n",
    "\n",
    "        # Compare predictions and answers, only where answer is 'true'\n",
    "        corrects = (predictions == 'true') & true_mask  # correct when answer is 'true' and prediction is 'true'\n",
    "\n",
    "        # Append results to category\n",
    "        category_corrects[category_name][\"corrects\"] += corrects.sum()\n",
    "        category_corrects[category_name][\"true_mask_count\"] += true_mask.sum()\n",
    "        category_corrects[\"Average\"]['corrects'] += corrects.sum()\n",
    "        category_corrects[\"Average\"]['true_mask_count'] += true_mask.sum()\n",
    "\n",
    "    # Compute accuracy per category: correct_true / total_true\n",
    "    results = {}\n",
    "    for category_name, record_dict in category_corrects.items():\n",
    "        acc = round(100 * (record_dict['corrects'] / record_dict['true_mask_count']), 4)\n",
    "        results[category_name] = acc\n",
    "\n",
    "    return results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compute subjects: 100%|██████████| 67/67 [00:00<00:00, 1445.11it/s]\n"
     ]
    }
   ],
   "source": [
    "benchmark_name = \"cmmlu\"\n",
    "folder = f\"/work/u5110390/BenchWeaver/score/trans_template_exp/{benchmark_name}/mix\"\n",
    "mapping_path = os.path.join(f\"/work/u5110390/BenchWeaver/evaluation_data/{benchmark_name}/mapping.json\")\n",
    "\n",
    "# load the JSON data    \n",
    "with open(mapping_path, 'r') as f:\n",
    "    mapping = json.load(f)\n",
    "    \n",
    "with open(os.path.join(folder, \"checked_answers.json\"), 'r') as f:\n",
    "    answer_data = json.load(f)\n",
    "    \n",
    "with open(os.path.join(folder, \"check_results.json\"), 'r') as f:\n",
    "    checked_data = json.load(f)\n",
    "    # retreive the answer\n",
    "    for subj, check_list in checked_data.items():\n",
    "        bool_list = []\n",
    "        for check in check_list:\n",
    "            bool_list.append(parse_bool_score(check))\n",
    "        checked_data[subj] = bool_list\n",
    "\n",
    "score_dict = compute_score(benchmark_name, answer_data, checked_data, mapping)\n",
    "\n",
    "with open(os.path.join(folder, \"score.json\"), 'w') as f:\n",
    "    json.dump(score_dict, f, indent=4, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "question_check_result_path = \"/work/u5110390/BenchWeaver/score/translation_results/kmmlu/mix/question_check_result.json\"\n",
    "answer_check_result_path = \"/work/u5110390/BenchWeaver/score/translation_results/kmmlu/mix/answer_check_result.json\"\n",
    "with open(question_check_result_path, 'r') as f:\n",
    "    question_check_result = json.load(f)\n",
    "    \n",
    "with open(answer_check_result_path, 'r') as f:\n",
    "    answer_check_result = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List\n",
    "import numpy as np\n",
    "def merge_and_calculate_results(\n",
    "    question_check_result: Dict[str, List[dict]],\n",
    "    answer_check_result: Dict[str, List[dict]]\n",
    "    ):\n",
    "    score_dict = {\n",
    "        subj: {\n",
    "            \"question\":{\n",
    "                \"資訊保留度\": 0,\n",
    "                \"風格匹配度\": 0,\n",
    "                \"專有名詞準確度\": 0,\n",
    "                \"翻譯品質\": 0,\n",
    "                \"Average\": 0\n",
    "            },\n",
    "            \"answer\":{\n",
    "                \"資訊保留度\": 0,\n",
    "                \"風格匹配度\": 0,\n",
    "                \"專有名詞準確度\": 0,\n",
    "                \"翻譯品質\": 0,\n",
    "                \"Average\": 0\n",
    "            }\n",
    "        } \n",
    "        for subj in question_check_result.keys()\n",
    "    }\n",
    "    average_score_dict = {\n",
    "        \"資訊保留度\": 0,\n",
    "        \"風格匹配度\": 0,\n",
    "        \"專有名詞準確度\": 0,\n",
    "        \"翻譯品質\": 0,\n",
    "        \"Average\": 0,\n",
    "        \"question\":{\n",
    "                \"資訊保留度\": 0,\n",
    "                \"風格匹配度\": 0,\n",
    "                \"專有名詞準確度\": 0,\n",
    "                \"翻譯品質\": 0,\n",
    "                \"Average\": 0\n",
    "            },\n",
    "            \"answer\":{\n",
    "                \"資訊保留度\": 0,\n",
    "                \"風格匹配度\": 0,\n",
    "                \"專有名詞準確度\": 0,\n",
    "                \"翻譯品質\": 0,\n",
    "                \"Average\": 0\n",
    "            }\n",
    "    }\n",
    "    \n",
    "    for subj in question_check_result.keys():\n",
    "        question_record_dict = {\n",
    "            \"資訊保留度\": [],\n",
    "            \"風格匹配度\": [],\n",
    "            \"專有名詞準確度\": [],\n",
    "            \"翻譯品質\": []\n",
    "        }\n",
    "        answer_record_dict = {\n",
    "            \"資訊保留度\": [],\n",
    "            \"風格匹配度\": [],\n",
    "            \"專有名詞準確度\": [],\n",
    "            \"翻譯品質\": []\n",
    "        }\n",
    "        for question_result_dict, answer_result_dict in zip(question_check_result[subj], answer_check_result[subj]):\n",
    "            # append the scores to the record dict\n",
    "            try:\n",
    "                question_record_dict[\"資訊保留度\"].append(question_result_dict[\"資訊保留度\"][\"分數\"])\n",
    "                question_record_dict[\"風格匹配度\"].append(question_result_dict[\"風格匹配度\"][\"分數\"])\n",
    "                question_record_dict[\"專有名詞準確度\"].append(question_result_dict[\"專有名詞準確度\"][\"分數\"])\n",
    "                question_record_dict[\"翻譯品質\"].append(question_result_dict[\"翻譯品質\"][\"分數\"])\n",
    "                answer_record_dict[\"資訊保留度\"].append(answer_result_dict[\"資訊保留度\"][\"分數\"])\n",
    "                answer_record_dict[\"風格匹配度\"].append(answer_result_dict[\"風格匹配度\"][\"分數\"])\n",
    "                answer_record_dict[\"專有名詞準確度\"].append(answer_result_dict[\"專有名詞準確度\"][\"分數\"])\n",
    "                answer_record_dict[\"翻譯品質\"].append(answer_result_dict[\"翻譯品質\"][\"分數\"])\n",
    "                # calculate the average score for each subject\n",
    "                score_dict[subj]['question']['資訊保留度'] = np.mean(question_record_dict[\"資訊保留度\"])\n",
    "                score_dict[subj]['question']['風格匹配度'] = np.mean(question_record_dict[\"風格匹配度\"])\n",
    "                score_dict[subj]['question']['專有名詞準確度'] = np.mean(question_record_dict[\"專有名詞準確度\"])\n",
    "                score_dict[subj]['question']['翻譯品質'] = np.mean(question_record_dict[\"翻譯品質\"])\n",
    "                score_dict[subj]['question']['Average'] = np.mean([\n",
    "                    score_dict[subj]['question']['資訊保留度'],\n",
    "                    score_dict[subj]['question']['風格匹配度'],\n",
    "                    score_dict[subj]['question']['專有名詞準確度'],\n",
    "                    score_dict[subj]['question']['翻譯品質']\n",
    "                ])\n",
    "                score_dict[subj]['answer']['資訊保留度'] = np.mean(answer_record_dict[\"資訊保留度\"])\n",
    "                score_dict[subj]['answer']['風格匹配度'] = np.mean(answer_record_dict[\"風格匹配度\"])\n",
    "                score_dict[subj]['answer']['專有名詞準確度'] = np.mean(answer_record_dict[\"專有名詞準確度\"])\n",
    "                score_dict[subj]['answer']['翻譯品質'] = np.mean(answer_record_dict[\"翻譯品質\"])\n",
    "                score_dict[subj]['answer']['Average'] = np.mean([\n",
    "                    score_dict[subj]['answer']['資訊保留度'],\n",
    "                    score_dict[subj]['answer']['風格匹配度'],\n",
    "                    score_dict[subj]['answer']['專有名詞準確度'],\n",
    "                    score_dict[subj]['answer']['翻譯品質']\n",
    "                ])\n",
    "            except Exception as e:\n",
    "                print(f\"Exception: {e} in subject {subj}\")\n",
    "                print(f\"Question result: {question_result_dict}\")\n",
    "                print(f\"Answer result: {answer_result_dict}\")\n",
    "                continue\n",
    "            \n",
    "    # calculate average score for each subject\n",
    "    average_score_dict['資訊保留度'] = np.mean(\n",
    "        [score_dict[subj]['question']['資訊保留度'] for subj in score_dict.keys()] + \n",
    "        [score_dict[subj]['answer']['資訊保留度'] for subj in score_dict.keys()]\n",
    "    )\n",
    "    average_score_dict['question'][\"資訊保留度\"] = np.mean(\n",
    "        [score_dict[subj]['question']['資訊保留度'] for subj in score_dict.keys()]\n",
    "    )\n",
    "    average_score_dict['answer'][\"資訊保留度\"] = np.mean(\n",
    "        [score_dict[subj]['answer']['資訊保留度'] for subj in score_dict.keys()]\n",
    "    )\n",
    "    average_score_dict['風格匹配度'] = np.mean(\n",
    "        [score_dict[subj]['question']['風格匹配度'] for subj in score_dict.keys()] + \n",
    "        [score_dict[subj]['answer']['風格匹配度'] for subj in score_dict.keys()]\n",
    "    )\n",
    "    average_score_dict['question'][\"風格匹配度\"] = np.mean(\n",
    "        [score_dict[subj]['question']['風格匹配度'] for subj in score_dict.keys()]\n",
    "    )\n",
    "    average_score_dict['answer'][\"風格匹配度\"] = np.mean(\n",
    "        [score_dict[subj]['answer']['風格匹配度'] for subj in score_dict.keys()]\n",
    "    )\n",
    "    average_score_dict['專有名詞準確度'] = np.mean(\n",
    "        [score_dict[subj]['question']['專有名詞準確度'] for subj in score_dict.keys()] + \n",
    "        [score_dict[subj]['answer']['專有名詞準確度'] for subj in score_dict.keys()]\n",
    "    )\n",
    "    average_score_dict['question'][\"專有名詞準確度\"] = np.mean(\n",
    "        [score_dict[subj]['question']['專有名詞準確度'] for subj in score_dict.keys()]\n",
    "    )\n",
    "    average_score_dict['answer'][\"專有名詞準確度\"] = np.mean(\n",
    "        [score_dict[subj]['answer']['專有名詞準確度'] for subj in score_dict.keys()]\n",
    "    )\n",
    "    average_score_dict['翻譯品質'] = np.mean(\n",
    "        [score_dict[subj]['question']['翻譯品質'] for subj in score_dict.keys()] + \n",
    "        [score_dict[subj]['answer']['翻譯品質'] for subj in score_dict.keys()]\n",
    "    )\n",
    "    average_score_dict['question'][\"翻譯品質\"] = np.mean(\n",
    "        [score_dict[subj]['question']['翻譯品質'] for subj in score_dict.keys()]\n",
    "    )\n",
    "    average_score_dict['answer'][\"翻譯品質\"] = np.mean(\n",
    "        [score_dict[subj]['answer']['翻譯品質'] for subj in score_dict.keys()]\n",
    "    )\n",
    "    average_score_dict['Average'] = np.mean([\n",
    "        average_score_dict['資訊保留度'],\n",
    "        average_score_dict['風格匹配度'],\n",
    "        average_score_dict['專有名詞準確度'],\n",
    "        average_score_dict['翻譯品質']\n",
    "    ])\n",
    "    average_score_dict['question'][\"Average\"] = np.mean([\n",
    "        average_score_dict['question'][\"資訊保留度\"],\n",
    "        average_score_dict['question'][\"風格匹配度\"],\n",
    "        average_score_dict['question'][\"專有名詞準確度\"],\n",
    "        average_score_dict['question'][\"翻譯品質\"]\n",
    "    ])\n",
    "    average_score_dict['answer'][\"Average\"] = np.mean([\n",
    "        average_score_dict['answer'][\"資訊保留度\"],\n",
    "        average_score_dict['answer'][\"風格匹配度\"],\n",
    "        average_score_dict['answer'][\"專有名詞準確度\"],\n",
    "        average_score_dict['answer'][\"翻譯品質\"]\n",
    "    ])\n",
    "    \n",
    "    score_dict.update({\"Average\": average_score_dict})\n",
    "    return score_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List\n",
    "import numpy as np\n",
    "\n",
    "def merge_and_calculate_results(\n",
    "    question_check_result: Dict[str, List[dict]],\n",
    "    answer_check_result: Dict[str, List[dict]]  # kept for compatibility, but will be unused\n",
    "):\n",
    "    score_dict = {\n",
    "        subj: {\n",
    "            \"question\": {\n",
    "                \"資訊保留度\": 0,\n",
    "                \"風格匹配度\": 0,\n",
    "                \"專有名詞準確度\": 0,\n",
    "                \"翻譯品質\": 0,\n",
    "                \"Average\": 0\n",
    "            }\n",
    "        }\n",
    "        for subj in question_check_result.keys()\n",
    "    }\n",
    "\n",
    "    average_score_dict = {\n",
    "        \"資訊保留度\": 0,\n",
    "        \"風格匹配度\": 0,\n",
    "        \"專有名詞準確度\": 0,\n",
    "        \"翻譯品質\": 0,\n",
    "        \"Average\": 0,\n",
    "        \"question\": {\n",
    "            \"資訊保留度\": 0,\n",
    "            \"風格匹配度\": 0,\n",
    "            \"專有名詞準確度\": 0,\n",
    "            \"翻譯品質\": 0,\n",
    "            \"Average\": 0\n",
    "        }\n",
    "    }\n",
    "\n",
    "    for subj in question_check_result.keys():\n",
    "        question_record_dict = {\n",
    "            \"資訊保留度\": [],\n",
    "            \"風格匹配度\": [],\n",
    "            \"專有名詞準確度\": [],\n",
    "            \"翻譯品質\": []\n",
    "        }\n",
    "\n",
    "        for question_result_dict in question_check_result[subj]:\n",
    "            try:\n",
    "                question_record_dict[\"資訊保留度\"].append(question_result_dict[\"資訊保留度\"][\"分數\"])\n",
    "                question_record_dict[\"風格匹配度\"].append(question_result_dict[\"風格匹配度\"][\"分數\"])\n",
    "                question_record_dict[\"專有名詞準確度\"].append(question_result_dict[\"專有名詞準確度\"][\"分數\"])\n",
    "                question_record_dict[\"翻譯品質\"].append(question_result_dict[\"翻譯品質\"][\"分數\"])\n",
    "\n",
    "                score_dict[subj]['question']['資訊保留度'] = np.mean(question_record_dict[\"資訊保留度\"])\n",
    "                score_dict[subj]['question']['風格匹配度'] = np.mean(question_record_dict[\"風格匹配度\"])\n",
    "                score_dict[subj]['question']['專有名詞準確度'] = np.mean(question_record_dict[\"專有名詞準確度\"])\n",
    "                score_dict[subj]['question']['翻譯品質'] = np.mean(question_record_dict[\"翻譯品質\"])\n",
    "                score_dict[subj]['question']['Average'] = np.mean([\n",
    "                    score_dict[subj]['question']['資訊保留度'],\n",
    "                    score_dict[subj]['question']['風格匹配度'],\n",
    "                    score_dict[subj]['question']['專有名詞準確度'],\n",
    "                    score_dict[subj]['question']['翻譯品質']\n",
    "                ])\n",
    "            except Exception as e:\n",
    "                print(f\"Exception: {e} in subject {subj}\")\n",
    "                print(f\"Question result: {question_result_dict}\")\n",
    "                continue\n",
    "\n",
    "    for metric in [\"資訊保留度\", \"風格匹配度\", \"專有名詞準確度\", \"翻譯品質\"]:\n",
    "        scores = [score_dict[subj]['question'][metric] for subj in score_dict.keys()]\n",
    "        average_score_dict[metric] = np.mean(scores)\n",
    "        average_score_dict['question'][metric] = average_score_dict[metric]\n",
    "\n",
    "    average_score_dict['Average'] = np.mean([\n",
    "        average_score_dict['資訊保留度'],\n",
    "        average_score_dict['風格匹配度'],\n",
    "        average_score_dict['專有名詞準確度'],\n",
    "        average_score_dict['翻譯品質']\n",
    "    ])\n",
    "    average_score_dict['question'][\"Average\"] = average_score_dict['Average']\n",
    "\n",
    "    score_dict.update({\"Average\": average_score_dict})\n",
    "    return score_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exception: '專有名詞準確度' in subject civil_engineering\n",
      "Question result: {'資訊保留度': {'分數': 10, '原因': '所有選擇題的問題和選項均被完整地翻譯並保留了原始資訊，包括關鍵細節、邏輯關係和語義準確性。'}, '風格匹配度': {'分數': 9, '原因': \"翻譯文本與風格範例中學術論文的語氣和形式相當一致，使用了正式且嚴謹的語言，但有部分措辭稍顯口語化。例如 'Which of the following...' 可以更正式地表達。\"}, '專有名詞准確度': {'分數': 10, '原因': \"所有專有名詞（如 'civil engineering', 'hydrographic surveys', 'standard penetration test' 等）均準確且一致地翻譯，符合集合慣例。\"}, '翻譯品質': {'分數': 10, '原因': '翻譯文本語法正確、流暢自然，完全避免了生硬直譯或機翻痕跡。句子結構良好且符合目標語言的語法規範。'}}\n",
      "Exception: '專有名詞準確度' in subject construction\n",
      "Question result: {'資訊保留度': {'分數': 9, '原因': '翻譯文本完整保留了原文的所有資訊，包括關鍵細節、邏輯關係和語義準確性。唯一的不足之處是翻譯中略去了原文中的部分上下文提示，如『사무소건축에서』。'}, '風格匹配度': {'分數': 8, '原因': '翻譯文本大體上符合風格範例的正式語氣和措辭選擇，但在句式排列上稍顯口語化。例如使用了『What is the correct』這種稍微更口語化的形式。'}, '專有名詞准確度': {'分數': 10, '原因': \"所有專有名詞均正確翻譯，並在英文和原文韓語之間保持一致，如 'Radiographic Test (방사선투과검사)' 等。\"}, '翻譯品質': {'分數': 9, '原因': \"翻譯文本流暢自然，語法規範，沒有生硬的直譯或機翻痕跡。唯一的稍微不足之處是個別句子（如 'In the case of continuously poured concrete'）中層次較多，稍顯複雜。\"}}\n",
      "Exception: '專有名詞準確度' in subject geomatics\n",
      "Question result: {'資訊保留度': {'分數': 7, '原因': '整體資訊內容和邏輯關係基本保留，但個別題目的翻譯文本存在不足，例如第二和第五題的翻譯不完整或要求提供額外資訊。'}, '風格匹配度': {'分數': 5, '原因': '翻譯文本與風格範例的正式語氣及措辭選擇存在一定差異，尤其是某些答題的解釋部分偏口語化，應保持一致的學術風格。'}, '專有名詞准確度': {'分數': 9, '原因': '大部分專有名詞翻譯準確，例如地理測量的術語和政府機構的名稱，符合標準翻譯慣例。'}, '翻譯品質': {'分數': 8, '原因': '翻譯的整體品質較好，語法通順自然，但部分翻譯略顯生硬，需要改善流暢度。'}}\n",
      "Exception: '專有名詞準確度' in subject machine_design_and_manufacturing\n",
      "Question result: {'資訊保留度': {'分數': 9, '原因': \"翻譯文本完整保留了原文的資訊內容，包括關鍵細節、邏輯關係及語義準確性。所有選擇題的問題和答案都被忠實地呈現出來。但在某一處翻譯中，'A' 項的內容未有提供足夠的上下文，導致該部分不完全。\"}, '風格匹配度': {'分數': 8, '原因': \"翻譯文本大致符合風格範例中的正式、嚴謹語氣及專業表達，與學術性材料的風格相符。然而，某些部分譯文稍微口語化了些，還有一部分翻譯有缺失內容，比如 'A' 項的翻譯不完整。\"}, '專有名詞准確度': {'分數': 10, '原因': \"專有名詞如 'plastic mold', 'sintered carbide', 'amorphous resin' 等翻譯準確且一致，符合標準翻譯慣例，並與上下文一致。\"}, '翻譯品質': {'分數': 9, '原因': '總體來說，翻譯文本自然流暢，語法正確且可讀性強。文中語句流暢，自然地反映了原文的意圖，避免了生硬直譯或機翻的痕跡。部分翻譯中缺少足夠的上下文，影響質量。'}}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accounting': {'question': {'資訊保留度': 7.14,\n",
       "   '風格匹配度': 7.49,\n",
       "   '專有名詞準確度': 8.25,\n",
       "   '翻譯品質': 6.92,\n",
       "   'Average': 7.449999999999999}},\n",
       " 'agricultural_sciences': {'question': {'資訊保留度': 9.16,\n",
       "   '風格匹配度': 8.35,\n",
       "   '專有名詞準確度': 9.175,\n",
       "   '翻譯品質': 8.62,\n",
       "   'Average': 8.82625}},\n",
       " 'aviation_engineering_and_maintenance': {'question': {'資訊保留度': 9.42,\n",
       "   '風格匹配度': 8.425,\n",
       "   '專有名詞準確度': 9.455,\n",
       "   '翻譯品質': 8.845,\n",
       "   'Average': 9.036249999999999}},\n",
       " 'biology': {'question': {'資訊保留度': 9.195,\n",
       "   '風格匹配度': 8.57,\n",
       "   '專有名詞準確度': 9.22,\n",
       "   '翻譯品質': 8.76,\n",
       "   'Average': 8.93625}},\n",
       " 'chemical_engineering': {'question': {'資訊保留度': 7.965,\n",
       "   '風格匹配度': 7.785,\n",
       "   '專有名詞準確度': 8.57,\n",
       "   '翻譯品質': 7.585,\n",
       "   'Average': 7.97625}},\n",
       " 'chemistry': {'question': {'資訊保留度': 9.185,\n",
       "   '風格匹配度': 8.365,\n",
       "   '專有名詞準確度': 9.33,\n",
       "   '翻譯品質': 8.71,\n",
       "   'Average': 8.8975}},\n",
       " 'civil_engineering': {'question': {'資訊保留度': 9.215,\n",
       "   '風格匹配度': 8.6,\n",
       "   '專有名詞準確度': 9.316582914572864,\n",
       "   '翻譯品質': 8.71356783919598,\n",
       "   'Average': 8.96128768844221}},\n",
       " 'computer_science': {'question': {'資訊保留度': 9.51,\n",
       "   '風格匹配度': 8.535,\n",
       "   '專有名詞準確度': 9.59,\n",
       "   '翻譯品質': 8.965,\n",
       "   'Average': 9.15}},\n",
       " 'construction': {'question': {'資訊保留度': 9.105,\n",
       "   '風格匹配度': 8.34,\n",
       "   '專有名詞準確度': 9.296482412060302,\n",
       "   '翻譯品質': 8.613065326633166,\n",
       "   'Average': 8.838636934673367}},\n",
       " 'criminal_law': {'question': {'資訊保留度': 8.28,\n",
       "   '風格匹配度': 8.5,\n",
       "   '專有名詞準確度': 8.955,\n",
       "   '翻譯品質': 8.02,\n",
       "   'Average': 8.438749999999999}},\n",
       " 'ecology': {'question': {'資訊保留度': 7.715,\n",
       "   '風格匹配度': 7.725,\n",
       "   '專有名詞準確度': 8.2,\n",
       "   '翻譯品質': 7.36,\n",
       "   'Average': 7.75}},\n",
       " 'economics': {'question': {'資訊保留度': 6.407692307692308,\n",
       "   '風格匹配度': 7.323076923076923,\n",
       "   '專有名詞準確度': 7.561538461538461,\n",
       "   '翻譯品質': 6.315384615384615,\n",
       "   'Average': 6.901923076923077}},\n",
       " 'education': {'question': {'資訊保留度': 5.57,\n",
       "   '風格匹配度': 6.75,\n",
       "   '專有名詞準確度': 7.2,\n",
       "   '翻譯品質': 5.47,\n",
       "   'Average': 6.2475}},\n",
       " 'electrical_engineering': {'question': {'資訊保留度': 8.145,\n",
       "   '風格匹配度': 7.98,\n",
       "   '專有名詞準確度': 8.705,\n",
       "   '翻譯品質': 7.77,\n",
       "   'Average': 8.149999999999999}},\n",
       " 'electronics_engineering': {'question': {'資訊保留度': 7.47,\n",
       "   '風格匹配度': 7.245,\n",
       "   '專有名詞準確度': 8.035,\n",
       "   '翻譯品質': 7.02,\n",
       "   'Average': 7.4425}},\n",
       " 'energy_management': {'question': {'資訊保留度': 7.525,\n",
       "   '風格匹配度': 7.4,\n",
       "   '專有名詞準確度': 8.115,\n",
       "   '翻譯品質': 7.14,\n",
       "   'Average': 7.545}},\n",
       " 'environmental_science': {'question': {'資訊保留度': 6.695,\n",
       "   '風格匹配度': 7.155,\n",
       "   '專有名詞準確度': 7.885,\n",
       "   '翻譯品質': 6.47,\n",
       "   'Average': 7.05125}},\n",
       " 'fashion': {'question': {'資訊保留度': 9.065,\n",
       "   '風格匹配度': 8.36,\n",
       "   '專有名詞準確度': 9.25,\n",
       "   '翻譯品質': 8.55,\n",
       "   'Average': 8.806249999999999}},\n",
       " 'food_processing': {'question': {'資訊保留度': 9.445,\n",
       "   '風格匹配度': 8.85,\n",
       "   '專有名詞準確度': 9.565,\n",
       "   '翻譯品質': 9.0,\n",
       "   'Average': 9.215}},\n",
       " 'gas_technology_and_engineering': {'question': {'資訊保留度': 7.855,\n",
       "   '風格匹配度': 7.765,\n",
       "   '專有名詞準確度': 8.465,\n",
       "   '翻譯品質': 7.42,\n",
       "   'Average': 7.876250000000001}},\n",
       " 'geomatics': {'question': {'資訊保留度': 7.94,\n",
       "   '風格匹配度': 7.875,\n",
       "   '專有名詞準確度': 8.638190954773869,\n",
       "   '翻譯品質': 7.57286432160804,\n",
       "   'Average': 8.006513819095478}},\n",
       " 'health': {'question': {'資訊保留度': 7.83,\n",
       "   '風格匹配度': 8.18,\n",
       "   '專有名詞準確度': 8.59,\n",
       "   '翻譯品質': 7.51,\n",
       "   'Average': 8.0275}},\n",
       " 'industrial_engineer': {'question': {'資訊保留度': 8.33,\n",
       "   '風格匹配度': 8.01,\n",
       "   '專有名詞準確度': 8.785,\n",
       "   '翻譯品質': 7.94,\n",
       "   'Average': 8.26625}},\n",
       " 'information_technology': {'question': {'資訊保留度': 9.34,\n",
       "   '風格匹配度': 8.415,\n",
       "   '專有名詞準確度': 9.36,\n",
       "   '翻譯品質': 8.895,\n",
       "   'Average': 9.0025}},\n",
       " 'interior_architecture_and_design': {'question': {'資訊保留度': 4.74,\n",
       "   '風格匹配度': 6.135,\n",
       "   '專有名詞準確度': 7.0,\n",
       "   '翻譯品質': 4.945,\n",
       "   'Average': 5.705}},\n",
       " 'law': {'question': {'資訊保留度': 7.055,\n",
       "   '風格匹配度': 7.84,\n",
       "   '專有名詞準確度': 8.205,\n",
       "   '翻譯品質': 6.98,\n",
       "   'Average': 7.5200000000000005}},\n",
       " 'machine_design_and_manufacturing': {'question': {'資訊保留度': 7.68,\n",
       "   '風格匹配度': 7.72,\n",
       "   '專有名詞準確度': 8.376884422110553,\n",
       "   '翻譯品質': 7.331658291457287,\n",
       "   'Average': 7.77713567839196}},\n",
       " 'management': {'question': {'資訊保留度': 7.77,\n",
       "   '風格匹配度': 8.15,\n",
       "   '專有名詞準確度': 8.67,\n",
       "   '翻譯品質': 7.565,\n",
       "   'Average': 8.03875}},\n",
       " 'maritime_engineering': {'question': {'資訊保留度': 8.505,\n",
       "   '風格匹配度': 8.12,\n",
       "   '專有名詞準確度': 8.72,\n",
       "   '翻譯品質': 8.155,\n",
       "   'Average': 8.375}},\n",
       " 'marketing': {'question': {'資訊保留度': 5.29,\n",
       "   '風格匹配度': 6.695,\n",
       "   '專有名詞準確度': 7.12,\n",
       "   '翻譯品質': 5.37,\n",
       "   'Average': 6.11875}},\n",
       " 'materials_engineering': {'question': {'資訊保留度': 8.625,\n",
       "   '風格匹配度': 8.145,\n",
       "   '專有名詞準確度': 8.99,\n",
       "   '翻譯品質': 8.13,\n",
       "   'Average': 8.4725}},\n",
       " 'mechanical_engineering': {'question': {'資訊保留度': 6.845,\n",
       "   '風格匹配度': 7.395,\n",
       "   '專有名詞準確度': 8.02,\n",
       "   '翻譯品質': 6.645,\n",
       "   'Average': 7.226249999999999}},\n",
       " 'nondestructive_testing': {'question': {'資訊保留度': 8.225,\n",
       "   '風格匹配度': 7.94,\n",
       "   '專有名詞準確度': 8.605,\n",
       "   '翻譯品質': 7.835,\n",
       "   'Average': 8.15125}},\n",
       " 'patent': {'question': {'資訊保留度': 7.57,\n",
       "   '風格匹配度': 8.07,\n",
       "   '專有名詞準確度': 8.45,\n",
       "   '翻譯品質': 7.16,\n",
       "   'Average': 7.8125}},\n",
       " 'political_science_and_sociology': {'question': {'資訊保留度': 5.71,\n",
       "   '風格匹配度': 7.245,\n",
       "   '專有名詞準確度': 7.23,\n",
       "   '翻譯品質': 5.75,\n",
       "   'Average': 6.483750000000001}},\n",
       " 'psychology': {'question': {'資訊保留度': 8.535,\n",
       "   '風格匹配度': 8.22,\n",
       "   '專有名詞準確度': 8.81,\n",
       "   '翻譯品質': 8.13,\n",
       "   'Average': 8.423750000000002}},\n",
       " 'public_safety': {'question': {'資訊保留度': 7.27,\n",
       "   '風格匹配度': 7.38,\n",
       "   '專有名詞準確度': 8.045,\n",
       "   '翻譯品質': 6.84,\n",
       "   'Average': 7.38375}},\n",
       " 'railway_and_automotive_engineering': {'question': {'資訊保留度': 8.08,\n",
       "   '風格匹配度': 7.9,\n",
       "   '專有名詞準確度': 8.715,\n",
       "   '翻譯品質': 7.615,\n",
       "   'Average': 8.0775}},\n",
       " 'real_estate': {'question': {'資訊保留度': 7.06,\n",
       "   '風格匹配度': 7.64,\n",
       "   '專有名詞準確度': 8.4,\n",
       "   '翻譯品質': 7.035,\n",
       "   'Average': 7.53375}},\n",
       " 'refrigerating_machinery': {'question': {'資訊保留度': 9.08,\n",
       "   '風格匹配度': 8.385,\n",
       "   '專有名詞準確度': 9.3,\n",
       "   '翻譯品質': 8.71,\n",
       "   'Average': 8.86875}},\n",
       " 'social_welfare': {'question': {'資訊保留度': 7.51,\n",
       "   '風格匹配度': 7.96,\n",
       "   '專有名詞準確度': 8.76,\n",
       "   '翻譯品質': 7.225,\n",
       "   'Average': 7.86375}},\n",
       " 'taxation': {'question': {'資訊保留度': 7.835,\n",
       "   '風格匹配度': 8.395,\n",
       "   '專有名詞準確度': 8.7,\n",
       "   '翻譯品質': 7.505,\n",
       "   'Average': 8.10875}},\n",
       " 'telecommunications_and_wireless_technology': {'question': {'資訊保留度': 8.18,\n",
       "   '風格匹配度': 7.955,\n",
       "   '專有名詞準確度': 8.765,\n",
       "   '翻譯品質': 7.74,\n",
       "   'Average': 8.16}},\n",
       " 'korean_history': {'question': {'資訊保留度': 6.84,\n",
       "   '風格匹配度': 7.67,\n",
       "   '專有名詞準確度': 7.65,\n",
       "   '翻譯品質': 6.88,\n",
       "   'Average': 7.26}},\n",
       " 'math': {'question': {'資訊保留度': 9.19,\n",
       "   '風格匹配度': 8.555,\n",
       "   '專有名詞準確度': 9.565,\n",
       "   '翻譯品質': 8.85,\n",
       "   'Average': 9.04}},\n",
       " 'Average': {'資訊保留度': 7.891170940170938,\n",
       "  '風格匹配度': 7.900179487179487,\n",
       "  '專有名詞準確度': 8.56910398144569,\n",
       "  '翻譯品質': 7.568478675428424,\n",
       "  'Average': 7.982233271056135,\n",
       "  'question': {'資訊保留度': 7.891170940170938,\n",
       "   '風格匹配度': 7.900179487179487,\n",
       "   '專有名詞準確度': 8.56910398144569,\n",
       "   '翻譯品質': 7.568478675428424,\n",
       "   'Average': 7.982233271056135}}}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score = merge_and_calculate_results(question_check_result, answer_check_result)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# export\n",
    "with open(\"/work/u5110390/BenchWeaver/score/translation_results/cmmlu/few_shot/score.json\", 'w') as f:\n",
    "    json.dump(score, f, indent=2, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "Parse score error: Score: [[3.5]]\n",
      "-1.0\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def parse_numerical_score(text: str) -> float:\n",
    "    score = -1.0\n",
    "    regex_patterns = [\n",
    "        r'(?:score|分數)\\s*[:：]?\\s*([\\d.]+)',\n",
    "        r'rating:\\s*\\[\\[([\\d.]+)\\]\\]$'\n",
    "    ]\n",
    "    for pattern in regex_patterns:\n",
    "        match = re.search(pattern, text, re.IGNORECASE)\n",
    "        if match:\n",
    "            try:\n",
    "                score = float(match.group(1))\n",
    "                break\n",
    "            except ValueError:\n",
    "                continue  # Skip to next pattern if conversion fails\n",
    "    else:\n",
    "        # This block runs if no break occurred (i.e., no match)\n",
    "        t = text.replace('\\n', '\\\\n')\n",
    "        print(f'Parse score error: {t}')\n",
    "        \n",
    "    return score\n",
    "\n",
    "print(parse_numerical_score(\"Score:\\n0.5\"))\n",
    "print(parse_numerical_score(\"分數：0.5\"))\n",
    "print(parse_numerical_score(\"分數： 0.5\"))\n",
    "print(parse_numerical_score(\"分數：0.5\\n\"))\n",
    "print(parse_numerical_score(\"分數：0.5\\n\"))\n",
    "print(parse_numerical_score(\"Score: [[3.5]]\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.mean([1.0, 2, 3, 4, 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'資訊保留度': {'分數': 4,\n",
       "  '原因': '回答中有部分資訊缺失',\n",
       "  'timestamp': '2025-04-13 20:02:39.104163'},\n",
       " '風格匹配度': {'分數': 3.5, '原因': None},\n",
       " '自定義物件': '{1, 2, 3}'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "input_data = {\n",
    "    \"資訊保留度\": {\n",
    "        \"分數\": 4,\n",
    "        \"原因\": \"回答中有部分資訊缺失\",\n",
    "        \"timestamp\": datetime.now(),  # Not JSON serializable\n",
    "    },\n",
    "    \"風格匹配度\": {\n",
    "        \"分數\": 3.5,\n",
    "        \"原因\": None\n",
    "    },\n",
    "    \"自定義物件\": set([1, 2, 3])  # Also not JSON serializable\n",
    "}\n",
    "def ensure_json_serializable(obj):\n",
    "    \"\"\"\n",
    "    Recursively ensure all values in the dict are JSON serializable.\n",
    "    \"\"\"\n",
    "    if isinstance(obj, dict):\n",
    "        return {str(k): ensure_json_serializable(v) for k, v in obj.items()}\n",
    "    elif isinstance(obj, list):\n",
    "        return [ensure_json_serializable(v) for v in obj]\n",
    "    elif isinstance(obj, (str, int, float, bool)) or obj is None:\n",
    "        return obj\n",
    "    else:\n",
    "        return str(obj)  # Fallback to string representation\n",
    "\n",
    "ensure_json_serializable(input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from typing import Any, Dict, List\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "os.environ[\"JAVA_HOME\"]=\"/usr/lib/java\"\n",
    "from BenchWeaver.eval.metric.retrieve_score import parse_bool_score, parse_numerical_score\n",
    "\n",
    "folder = \"/work/u5110390/BenchWeaver/score/trans_template_exp/tmmluplus/mix\"\n",
    "with open(\"/work/u5110390/BenchWeaver/evaluation_data/tmmluplus/mapping.json\", \"r\") as f:\n",
    "    catagories = json.load(f)\n",
    "    for key, value in catagories.items():\n",
    "        value[\"category\"] = key\n",
    "            \n",
    "with open(os.path.join(folder, \"checked_answers.json\"), 'r') as f:\n",
    "    checked_answers = json.load(f)\n",
    "with open(os.path.join(folder, \"check_results.json\"), 'r') as f:\n",
    "    check_results = json.load(f)\n",
    "    \n",
    "def retrieve_answer(text: str, numerical:bool=False) -> str | float:\n",
    "        return parse_numerical_score(text) if numerical else parse_bool_score(text)\n",
    "    \n",
    "def comput_score(catagories, checked_answers: Dict[str, List[Any]], check_results: Dict[str, List[Any]], subjects: List[str]) -> Dict[str, float]:\n",
    "        category_corrects = {score: {\"corrects\": 0, \"true_mask_count\": 0} for score in subjects}\n",
    "\n",
    "        for subject in tqdm(catagories.keys(), desc=\"Compute subjects\"):\n",
    "            category_name = catagories[subject][\"category\"]\n",
    "            answers = np.array(checked_answers[subject])\n",
    "            predictions = np.array([retrieve_answer(ans) for ans in check_results[subject]])\n",
    "            # Mask for when the answer is 'true'\n",
    "            true_mask: np.ndarray = answers == 'true'\n",
    "            # Compare predictions and answers, only where answer is 'true'\n",
    "            corrects: np.ndarray = (predictions == 'true') & true_mask  # correct when answer is 'true' and prediction is 'true'\n",
    "            # Update the corrects and true_mask counts\n",
    "            category_corrects[category_name][\"corrects\"] += corrects.sum()\n",
    "            category_corrects[category_name][\"true_mask_count\"] += true_mask.sum()\n",
    "            category_corrects[\"Average\"]['corrects'] += corrects.sum()\n",
    "            category_corrects[\"Average\"]['true_mask_count'] += true_mask.sum()\n",
    "        \n",
    "        return {\n",
    "            category_name: round(100 * (record_dict['corrects'] / record_dict['true_mask_count']), 4)\n",
    "                for category_name, record_dict in category_corrects.items()\n",
    "        }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compute subjects: 100%|██████████| 66/66 [00:00<00:00, 403.33it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Average': 44.6168,\n",
       " 'dentistry': 42.8571,\n",
       " 'traditional_chinese_medicine_clinical_medicine': 31.6547,\n",
       " 'clinical_psychology': 61.6,\n",
       " 'technical': 49.7512,\n",
       " 'culinary_skills': 54.1096,\n",
       " 'mechanical': 53.3898,\n",
       " 'logic_reasoning': 30.2158,\n",
       " 'real_estate': 45.6522,\n",
       " 'general_principles_of_law': 39.6226,\n",
       " 'finance_banking': 47.4074,\n",
       " 'anti_money_laundering': 62.6866,\n",
       " 'ttqav2': 63.7168,\n",
       " 'marketing_management': 70.9677,\n",
       " 'business_management': 53.9568,\n",
       " 'organic_chemistry': 60.5505,\n",
       " 'advance_chemistry': 40.6504,\n",
       " 'physics': 54.6392,\n",
       " 'secondary_physics': 63.3929,\n",
       " 'human_behavior': 57.6052,\n",
       " 'national_protection': 45.0237,\n",
       " 'jce_humanities': 4.4444,\n",
       " 'politic_science': 53.7688,\n",
       " 'agriculture': 50.3311,\n",
       " 'official_document_management': 36.4865,\n",
       " 'financial_analysis': 58.377,\n",
       " 'pharmacy': 39.6419,\n",
       " 'educational_psychology': 62.5,\n",
       " 'statistics_and_machine_learning': 51.7857,\n",
       " 'management_accounting': 33.9535,\n",
       " 'introduction_to_law': 3.7975,\n",
       " 'computer_science': 60.3448,\n",
       " 'veterinary_pathology': 50.1767,\n",
       " 'accounting': 0.5236,\n",
       " 'fire_science': 38.7097,\n",
       " 'optometry': 39.0217,\n",
       " 'insurance_studies': 43.1579,\n",
       " 'pharmacology': 49.2201,\n",
       " 'taxation': 26.6667,\n",
       " 'education_(profession_level)': 40.535,\n",
       " 'economics': 65.6489,\n",
       " 'veterinary_pharmacology': 58.3333,\n",
       " 'nautical_science': 29.9456,\n",
       " 'occupational_therapy_for_psychological_disorders': 58.7477,\n",
       " 'trust_practice': 37.4065,\n",
       " 'geography_of_taiwan': 52.6042,\n",
       " 'physical_education': 44.1341,\n",
       " 'auditing': 41.6364,\n",
       " 'administrative_law': 30.9524,\n",
       " 'basic_medical_science': 60.1677,\n",
       " 'macroeconomics': 55.9611,\n",
       " 'trade': 36.8526,\n",
       " 'chinese_language_and_literature': 30.1508,\n",
       " 'tve_design': 54.375,\n",
       " 'junior_science_exam': 66.1972,\n",
       " 'junior_math_exam': 33.7143,\n",
       " 'junior_chinese_exam': 3.4286,\n",
       " 'junior_social_studies': 21.4286,\n",
       " 'tve_mathematics': 25.3333,\n",
       " 'tve_chinese_language': 0.207,\n",
       " 'tve_natural_sciences': 64.1509,\n",
       " 'junior_chemistry': 39.2344,\n",
       " 'music': 31.295,\n",
       " 'education': 42.7419,\n",
       " 'three_principles_of_people': 61.1511,\n",
       " 'taiwanese_hokkien': 17.0543,\n",
       " 'engineering_math': 39.8058}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subjects = ['Average'] + [_ for _ in catagories.keys()]\n",
    "comput_score(catagories, checked_answers, check_results, subjects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from BenchWeaver.eval.benchmarks.en.ifeval.source_code.evaluation_main import evaluate_instruction_following\n",
    "import json\n",
    "input_data = \"/work/u5110390/BenchWeaver/instruction_following_eval/data/input_data.jsonl\"\n",
    "input_response_data = \"/work/u5110390/BenchWeaver/instruction_following_eval/data/input_response_data_gpt4_20231107_145030.jsonl\"\n",
    "output_dir = \"/work/u5110390/BenchWeaver/scripts\"\n",
    "\n",
    "print(json.dumps(\n",
    "evaluate_instruction_following(\n",
    "    input_data=input_data,\n",
    "    input_response_data=input_response_data,\n",
    "    output_dir=output_dir,\n",
    "), ensure_ascii=False, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'num_highlights': 3.0,\n",
       " 'relation': 'at least',\n",
       " 'num_words': 300.0,\n",
       " 'num_placeholders': None,\n",
       " 'prompt_to_repeat': None,\n",
       " 'num_bullets': None,\n",
       " 'section_spliter': None,\n",
       " 'num_sections': None,\n",
       " 'capital_relation': None,\n",
       " 'capital_frequency': None,\n",
       " 'keywords': None,\n",
       " 'num_paragraphs': None,\n",
       " 'language': None,\n",
       " 'let_relation': None,\n",
       " 'letter': None,\n",
       " 'let_frequency': None,\n",
       " 'end_phrase': None,\n",
       " 'forbidden_words': None,\n",
       " 'keyword': None,\n",
       " 'frequency': None,\n",
       " 'num_sentences': None,\n",
       " 'postscript_marker': None,\n",
       " 'first_word': None,\n",
       " 'nth_paragraph': None}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\n",
    "                path=\"/work/u5110390/BenchWeaver/evaluation_data/ifeval\",\n",
    "                name=\"all\",\n",
    "                cache_dir=None,\n",
    "                trust_remote_code=True,\n",
    "            )\n",
    "dataset['test'][0]['kwargs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['correlated', 'experiencing']\n",
      "['rock']\n",
      "['nourriture']\n",
      "['mom', 'mother']\n",
      "['field', 'thanks', 'issue', 'collaborator']\n",
      "['Argentinian']\n",
      "['nickname']\n",
      "['waste', 'material', 'meal']\n",
      "['sleep', 'cook', 'feed']\n",
      "['intern', 'grow']\n",
      "['brilliant', 'le', 'hou']\n",
      "['vulnerable']\n",
      "['sarah']\n",
      "['dupage', 'dade']\n",
      "['cat']\n",
      "['coop', 'killings', 'dead', 'night']\n",
      "['taylor', 'swift', 'together']\n",
      "['name', 'rename']\n",
      "['forests', 'riddle']\n",
      "['netflix']\n",
      "['atlantis', 'constable']\n",
      "['land', 'river']\n",
      "['can', 'ride']\n",
      "['station']\n",
      "['python', 'java']\n",
      "['heute']\n",
      "['moser', 'glassworks', 'pravcice', 'karlovy', 'vary']\n",
      "['disgusting', 'delicious', 'bad', 'good']\n",
      "['lacking', 'model', 'performance', 'quality', 'architecture']\n",
      "['calculate', 'file', 'conclusion']\n",
      "['bad', 'underperform']\n",
      "['yes', 'no']\n",
      "['enzymes', 'antibodies']\n",
      "['rich', 'money']\n",
      "['die']\n",
      "['rate', 'rte']\n",
      "['reschedule', 'free']\n",
      "['talented', 'tianjin']\n",
      "['ours', 'have']\n",
      "['startup', 'capsule']\n",
      "['flea', 'json']\n",
      "['died', 'drowned']\n",
      "['sad', 'crazy', 'stress']\n",
      "['ink', 'memoirs']\n",
      "['schlau']\n",
      "['gao', 'hearts']\n",
      "['finale', 'less']\n",
      "['BC', 'culture', 'prehistoric']\n",
      "['medalist', 'theta']\n",
      "['friends', 'hanson']\n",
      "['zelda', 'hyrule', 'link', 'ganon']\n",
      "['revolution', 'tax']\n",
      "['engages', 'lightly']\n",
      "['adoption', 'carriage']\n",
      "['yo', 'peace', 'check']\n",
      "['parody']\n",
      "['nursery', 'storytelling']\n",
      "['slow', 'like', 'kid']\n",
      "['story', 'killer', 'dead', 'found', 'law', 'room', 'kill', 'result', 'use', 'approach', 'people', 'president']\n",
      "['bark', 'run']\n",
      "['trust', 'brand', 'customer', 'law', 'policy', 'unusable']\n",
      "['steps', 'step']\n",
      "['climate', 'energy', 'green']\n",
      "['indicator', 'management']\n",
      "['university']\n",
      "['education']\n",
      "['dammit']\n",
      "['kill', 'slaughter', 'occupy', 'invasion']\n",
      "['mileage', 'fuel']\n",
      "['farmer']\n",
      "['economy', 'demand', 'supply']\n",
      "['afternoon', 'distressed']\n",
      "['people', 'skills']\n",
      "['remainder']\n",
      "['divide', 'answer']\n",
      "['icefrog', 'blizzard', 'lawsuit']\n",
      "['bill', 'economist', 'jurgen']\n",
      "['computer', 'science']\n",
      "['name', 'song', 'person', 'man', 'woman']\n",
      "['proposal', 'project']\n",
      "['compensated', 'immigrants']\n",
      "['jock']\n",
      "['beauty', 'pretty']\n",
      "['perfume', 'fresh', 'good']\n",
      "['key']\n",
      "['festival', 'river']\n",
      "['youngins', 'damn']\n",
      "['dog', 'day']\n"
     ]
    }
   ],
   "source": [
    "for dict in dataset['test']:\n",
    "    if dict['kwargs']['keywords'] is not None:\n",
    "        print(dict['kwargs']['keywords'])\n",
    "    \n",
    "    if dict['kwargs']['forbidden_words'] is not None:\n",
    "        print(dict['kwargs']['forbidden_words'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BenchWeaver",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
