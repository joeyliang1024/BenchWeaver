# Copyright 2024 the LlamaFactory team.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

import os
from dataclasses import dataclass, field
from typing import Literal, Optional

from datasets import DownloadMode


@dataclass
class EvaluationArguments:
    r"""
    Arguments pertaining to specify the evaluation parameters.
    """

    task: str = field(
        metadata={"help": "Name of the evaluation task."},
    )
    ref_task: Optional[str] = field(
        default=None,
        metadata={"help": "Name of the reference task."},
    )
    task_dir: str = field(
        default="evaluation",
        metadata={"help": "Path to the folder containing the evaluation datasets."},
    )
    batch_size: int = field(
        default=4,
        metadata={"help": "The batch size per GPU for evaluation."},
    )
    seed: int = field(
        default=42,
        metadata={"help": "Random seed to be used with data loaders."},
    )
    lang: Literal["en", "zh", "zh-tw", "ko"] = field(
        default="en",
        metadata={"help": "Language used at evaluation."},
    )
    n_shot: int = field(
        default=5,
        metadata={"help": "Number of examplars for few-shot learning."},
    )
    save_dir: Optional[str] = field(
        default=None,
        metadata={"help": "Path to save the evaluation results."},
    )
    download_mode: DownloadMode = field(
        default=DownloadMode.REUSE_DATASET_IF_EXISTS,
        metadata={"help": "Download mode used for the evaluation datasets."},
    )
    system_prompt: str = field(
        default=None,
        metadata={"help": "The system prompt for the open question inference model."}
    )
    user_prompt: str = field(
        default=None,
        metadata={"help": "The user-provided prompt or query  for the open question inference model."}
    )
    criteria_system_prompt: str = field(
        default=None,
        metadata={"help": "The system prompt for the open question checker model."}
    )
    criteria_prompt: str = field(
        default=None,
        metadata={"help": "The user-defined criteria or guidelines for the open question checker model."}
    )
    cot: bool = field(
        default=False,
        metadata={"help": "Enable or disable chain of thought reasoning to enhance the open question inference model's response quality."}
    )
    benchmark_mode: Literal["mcqa-prob", "mcqa-oq", "opqa", "mix"] = field(
        default="opqa",
        metadata={"help": "Evaluation mode."},
    )
    pipeline: Literal["same", "diff"] = field(
        default=None,
        metadata={"help": "Indicate whether to run the same language or different language evaluation."}
    )
    testing_size: Optional[int] = field(
        default=1_000_000_000,
        metadata={"help": "Number of examples to evaluate on. If None, evaluate on the entire dataset."}
    )
    record_all: bool = field(
        default=False,
        metadata={"help": "Record all the intermediate steps of the reasoning process."}
    )
    
    def __post_init__(self):
        if self.save_dir is not None and os.path.exists(self.save_dir):
            raise ValueError("`save_dir` already exists, use another one.")
